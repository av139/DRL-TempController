20240424v1
done 1 当前是固定target temp。需要改成随机target temp，通过args传入target temp的上下界，通过随机数在每次episode生成随机整数target temp。也就是episode 1内target temp始终相同。episode 2内target temp就要不同于episode 1 时期的target temp。
done 2 增加定期评估eval-freq 以及定期保存save-freq 默认都是5000steps
done 3 done条件放松，超过目标温度15度需要保持150步才判定失败，保持在目标温度0.3以内也是需要保持150步才算done = true
done 4 episode 最大步长为30分钟，也就是900步。
done 5 warmup步数当前为1000 保持不变
done 6 每次reset时，在action区间内随机选择一个动作，执行30秒。
done 7 给train和evaluate做csv输出



20240425v1：
1 reward标准化到[-1,0]区间
2 修正csv的输出问题
3 warmupstep = 1000， episodestep = 1000
4 reset sleep time = 30s
5 增加 usd_sde

20240501v1:
1 增加tensorboard的输出语句【未完成】
2 episode_max_steps改成500
3 mqtt_pi增加实时显示语句
4 csv增加时间戳

20240501v2:（放弃，采用直接读取tensorboard的输出）
1 将verbose的输出保存至csv中，用于查看每个episode时的actor_loss, critic_loss, ent_coef, ent_coef_loss

20240506v1：
total_timesteps 50000
learning_rate 3e-4

20240508v1：
total_timesteps 80000
learning_rate 1e-4
max_temp_diff 15
target_temp_min 25
target_temp_max 50
verbose 3
use_sde true
use_sde_at_warmup true
self.observation_space = spaces.Box(low=np.array([25, 15, 15], dtype=np.float32),
                                    high=np.array([50, 60, 60], dtype=np.float32))

其他不变

20240509v1：
learning_rate 1e-4
在env.reset中增加语句：
self.steps_outside_critical_range = 0
修正缺少初始化的变量

20240511v1：
学习率至 1e-4
1 把args更新成config.json导入
更新target temp的选择算法
2 每次episode的随机target 温度在当前温度的正负5度到正负15度以内。且位于最大和最小区间内
2.1 例如当前温度30度，那么随机的范围就是15-25以及35-45之间，避免直接摇出一个当前温度

20240514v1:
学习率3e-4
在env的print中增加一个时间戳的print
其余不变
训练被中断，树莓派重启了

20240516v1:
同20240514v1，针对中断的训练重新开始，研究不同学习率对于结果的影响

20240519v1：
基于20240511v1，学习率改回1e-4，并关闭所有gSDE，研究gSDE对于结果的影响

20240522v1
学习率1e-4，打开所有gSDE，但是eval_freq改成2500，把evaluate的频率翻一倍，更频繁的评价智能体


大更新：
1 把env中关于设置target temp的语句单独打包成choose_target_temp，送进env_utils里
2 把env中compute_reward，check_done，update_temperature_counters单独提出来形成一个新的py文件 "env_utils.py"
2.1 去除原有env中的这三个类，使代码简洁。
3 加入对pid的支持
4 从json切换至yaml
5 加入ppo
6 把envsac更新成envsb3
7 在episode的csv输出内显示第一次进入温度范围区间的步数

20240524v1
使用ppo
按照当前温差奖励，按照ppo的默认参数训练
ppo:
  total_timesteps: 80000                               # 训练过程中的总时间步数
  episode_max_steps: 500                               # 每个环节的最大步数
  eval_freq: 5000                                      # 评估模型的频率（每多少步进行一次评估）
  save_freq: 5000                                      # 模型保存频率（每多少步保存一次模型）
  learning_rate: 0.0003                                # 默认值：0.0003    # 学习率
  n_steps: 2048                                        # 默认值：2048      # 更新策略之前收集的样本数量
  batch_size: 64                                       # 默认值：64        # 批次大小
  n_epochs: 10                                         # 默认值：10        # 每次更新时的优化轮数
  gamma: 0.99                                          # 默认值：0.99      # 折扣因子
  gae_lambda: 0.95                                     # 默认值：0.95      # GAE lambda
  clip_range: 0.2                                      # 默认值：0.2       # 剪切范围
  clip_range_vf: null                                  # 默认值：null      # 值函数的剪切范围
  normalize_advantage: true                            # 默认值：true      # 标准化优势函数
  ent_coef: 0.0                                        # 默认值：0.0       # 熵系数
  vf_coef: 0.5                                         # 默认值：0.5       # 值函数损失系数
  max_grad_norm: 0.5                                   # 默认值：0.5       # 最大梯度范数
  use_sde: false                                       # 默认值：false     # 使用状态依赖探索
  sde_sample_freq: -1                                  # 默认值：-1        # 状态依赖探索的采样频率
  rollout_buffer_class: null                           # 默认值：null      # 回放缓冲区类
  rollout_buffer_kwargs: null                          # 默认值：null      # 回放缓冲区参数
  target_kl: null                                      # 默认值：null      # 目标KL散度
  stats_window_size: 100                               # 默认值：100       # 统计窗口大小
  tensorboard_log: "./tensorboard_ppo/"                # 默认值：null      # Tensorboard日志路径
  policy_kwargs: null                                  # 默认值：null      # 策略参数
  verbose: 3                                           # 默认值：0         # 日志详细程度（0：无输出，1：偶尔输出，2：每步都输出，3：更详细的输出）
  seed: null                                           # 默认值：null      # 随机种子
  device: "auto"                                       # 默认值：“auto”    # 使用的设备（cpu, cuda, auto）


20240524v2
verbose 默认值 2

20240527 大更新！！！:
增加discrete离散环境
增加dqn代码
增加monitor包装器
给pid增加tensorboard输出
给所有evalcallback增加n_eval_episodes，即每次evaluate执行多少次episode
将sb3算法调用前创建环境的语句合并至EnvUtils.create_sb3_env

我现在在env utils中计算reward的方式是通过计算actual与target的温度差的负绝对值来定义。
我现在希望引入时间来定义reward
新的reward将由 当前的温差reward（范围[-1, 0]） + 时间reward（范围[-1, 0]） 组成

时间reward由以下的规则构成：
如果actual temp在taget temp的正负0.3范围内，则time reward = 0
如果actual temp超过这个0.3的范围，则每离开这个范围的step 就 叠加一个 -1/episode_max_steps 的reward
比如说在当前config的情况下，每个episode最大是500步，也就是每一个time reward就是 -0.002
比如说：
当step1 actual 在0.3 范围内，则step1的time reward = 0
当step2 actual 在0.3 范围内，则step2的time reward = 0
当step3 actual 在0.3 范围内，则step3的time reward = 0
当step4 actual 在0.3 范围外，则step4的time reward = -0.002
当step5 actual 在0.3 范围外，则step5的time reward = -0.004
当step6 actual 在0.3 范围外，则step6的time reward = -0.006
当step7 actual 在0.3 范围外，则step7的time reward = -0.008
当step8 actual 回到0.3 范围内，则step8的time reward = 0
当step9 actual 在0.3 范围内，则step9的time reward = 0
当step10 actual 在0.3 范围外，则step10的time reward = -0.010
当step11 actual 在0.3 范围外，则step11的time reward = -0.012
当step12 actual 在0.3 范围外，则step12的time reward = -0.014

total_reward = 0.6 * temp_reward + 0.4 * time_reward
以及分配了加权，目前4/6开

在config中创建temp_config区块，将对应的数值均改成参数化
加上了对于a2c DDPG TD3 TRPO的支持
加了对于fuzzy和random的基准方法的支持

增加了reset get temp的等待流程
避免get到0,0


20240527v1：
使用random方法进行基准评价测试

--------------------------------
功能更新：
增加dist_config
在evaluate中调用dist pwm，在train中不使用dist pwm
dist pwm的信号选择由dist_min / dist_max / chance_of_zero组成


3 减少每轮的step数量（500->250）
4 增加reward奖励 当进入目标温度正负0.3度时+0.5？
5 增加与时间相关的奖励与惩罚